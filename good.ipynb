{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c2ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Colab, nothing else is needed.\n"
     ]
    }
   ],
   "source": [
    "def is_colab():\n",
    "    \"\"\"Check if the code is running in Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Use the function to conditionally run magic commands\n",
    "if is_colab():\n",
    "    # Run Colab-specific magic commands\n",
    "    print(\"Running in Colab, executing magic commands.\")\n",
    "    !rm -rf microproyecto3NLP/\n",
    "    !git clone https://github.com/cjohana031/microproyecto3NLP\n",
    "    !cp -R microproyecto3NLP/* .\n",
    "    # Add any other Colab-specific setup\n",
    "else:\n",
    "    # Alternative setup for non-Colab environments\n",
    "    print(\"Not running in Colab, nothing else is needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8856986e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c5b6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juanc/Projects/claudia/microproyecto3NLP/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token found in environment variable.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Check HF_TOKEN environment variable\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    print(\"Hugging Face token found in environment variable.\")\n",
    "else:\n",
    "    token = input(\"Enter your Hugging Face token: \")\n",
    "    login(token=token) # Token de Hugging Face\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Configurar tokenizador\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6069d02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 87866/87866 [00:00<00:00, 112294.12 examples/s]\n",
      "Filter: 100%|██████████| 4934/4934 [00:00<00:00, 102464.71 examples/s]\n",
      "Filter: 100%|██████████| 4887/4887 [00:00<00:00, 104409.47 examples/s]\n",
      "Map: 100%|██████████| 8930/8930 [00:06<00:00, 1286.68 examples/s]\n",
      "Map: 100%|██████████| 480/480 [00:00<00:00, 1287.40 examples/s]\n",
      "Map: 100%|██████████| 514/514 [00:00<00:00, 1309.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8930/8930 [00:00<00:00, 421151.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 480/480 [00:00<00:00, 119368.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 514/514 [00:00<00:00, 84164.44 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['example_id', 'article', 'answer', 'question', 'options', 'instruction', 'completion', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8930\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['example_id', 'article', 'answer', 'question', 'options', 'instruction', 'completion', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 480\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['example_id', 'article', 'answer', 'question', 'options', 'instruction', 'completion', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 514\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import os\n",
    "def format_example(example):\n",
    "    article = example[\"article\"]\n",
    "    question = example[\"question\"]\n",
    "    options = example[\"options\"]\n",
    "    answer_idx = ord(example[\"answer\"]) - ord(\"A\")\n",
    "    \n",
    "    # Format options\n",
    "    formatted_options = \"\"\n",
    "    for i, opt in enumerate(options):\n",
    "        option_letter = chr(65 + i)  # A, B, C, D\n",
    "        formatted_options += f\"{option_letter}. {opt}\\n\"\n",
    "    \n",
    "    # Create instruction format for fine-tuning\n",
    "    instruction = f\"\"\"Read the following passage and answer the question by choosing the correct option.\n",
    "\n",
    "Passage:\n",
    "{article}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "{formatted_options}\n",
    "\n",
    "The correct answer is:\"\"\"\n",
    "    \n",
    "    # Create completion (what the model should generate)\n",
    "    completion = f\" {chr(65 + answer_idx)}\"\n",
    "    \n",
    "    example[\"instruction\"] = instruction\n",
    "    example[\"completion\"] = completion\n",
    "    return example\n",
    "\n",
    "def prepare_datasets(datasetdict: DatasetDict, tokenizer: AutoTokenizer) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Prepares the datasets for training by tokenizing the inputs and labels.\n",
    "    \"\"\"\n",
    "    def tokenize_function(example):\n",
    "               # Format the example to get instruction and completion\n",
    "        example = format_example(example)\n",
    "        instruction = example[\"instruction\"]\n",
    "        completion = example[\"completion\"]\n",
    "        \n",
    "        # First tokenize just the instruction to know its length\n",
    "        instruction_tokens = tokenizer(instruction, return_length=True)\n",
    "        instruction_length = instruction_tokens['length'][0]  # Get the length of instruction tokens\n",
    "        \n",
    "        # Then tokenize the full text (instruction + completion)\n",
    "        full_tokens = tokenizer(instruction , completion)\n",
    "        \n",
    "        # Set up the labels with -100 for instruction tokens, and actual token IDs for completion tokens\n",
    "        labels = full_tokens[\"input_ids\"].copy()\n",
    "        \n",
    "        # Mask out the instruction part with -100 (these won't contribute to loss)\n",
    "        labels[:instruction_length] = [-100] * instruction_length\n",
    "        \n",
    "        # Mask padding tokens with -100\n",
    "        labels[full_tokens[\"attention_mask\"] == 0] = -100\n",
    "        \n",
    "        # Store all in the example\n",
    "        example[\"input_ids\"] = full_tokens[\"input_ids\"]\n",
    "        example[\"attention_mask\"] = full_tokens[\"attention_mask\"]\n",
    "        example[\"labels\"] = labels\n",
    "        return example\n",
    "\n",
    "    # Apply the tokenization function to each split of the dataset\n",
    "    tokenized_datasets = datasetdict.map(tokenize_function, batched=False)\n",
    "    return tokenized_datasets\n",
    "\n",
    "def load_dataset_from_parquet(tokenizer: AutoTokenizer) -> DatasetDict:\n",
    "    data_files = {\"train\": \"data/train-00000-of-00001.parquet\", \"test\": \"data/test-00000-of-00001.parquet\", \"validation\": \"data/validation-00000-of-00001.parquet\"}\n",
    "    race = load_dataset(\"parquet\", data_files=data_files)\n",
    "    race = race.filter(lambda x: len(x['article']) < 800)\n",
    "    race : DatasetDict = prepare_datasets(race,tokenizer=tokenizer)\n",
    "    race.save_to_disk('data/datasets', max_shard_size=\"100MB\")\n",
    "    return race\n",
    "\n",
    "if not os.path.exists('data/datasets'):\n",
    "    race = load_dataset_from_parquet(tokenizer=base_tokenizer)\n",
    "else:\n",
    "    race = DatasetDict.load_from_disk('data/datasets')\n",
    "race"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
